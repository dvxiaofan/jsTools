/**
 * è„šæœ¬åç§°: Scan Artists Duplicates (æ‰¹é‡æ­Œæ‰‹ç›®å½•é‡å¤æ£€æµ‹)
 * åŠŸèƒ½æè¿°: æ‰¹é‡æ‰«ææŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼ˆå‡è®¾ä¸ºæ­Œæ‰‹åï¼‰ï¼Œç”Ÿæˆè¯¦ç»†çš„é‡å¤æ­Œæ›²æŠ¥å‘Šã€‚
 *          ä¼šè‡ªåŠ¨è·³è¿‡ééŸ³é¢‘æ–‡ä»¶ã€‚
 * ä½¿ç”¨æ–¹æ³•:
 *    node scan_artists_duplicates.js [æ ¹ç›®å½•] [æŠ¥å‘Šè¾“å‡ºè·¯å¾„]
 * ç¤ºä¾‹:
 *    node scan_artists_duplicates.js "/Volumes/Music/æ­Œæ‰‹åˆ†ç±»" "./my_report.txt"
 */

const fs = require('fs');
const path = require('path');
const { findFiles, getFileHash, parseSongInfo, getScore } = require('./check_duplicates_utils');

const rootDir = process.argv[2] || '/Volumes/Music/æ­Œæ‰‹åˆ†ç±»';
const reportFile = process.argv[3] || 'all_artists_duplicates_report_v2.txt';

// Artists to skip
const skipArtists = ['å¼ éŸ¶æ¶µ', 'å‘¨æ°ä¼¦'];

// ---------------------------------------------------------
// Main Logic
// ---------------------------------------------------------

async function run() {
    console.log(`ğŸš€ å¼€å§‹æ‰¹é‡æ‰«æ: ${rootDir}`);

    if (!fs.existsSync(rootDir)) {
        console.error('âŒ æ ¹ç›®å½•ä¸å­˜åœ¨');
        return;
    }

    // Get all artist folders
    const items = fs.readdirSync(rootDir, { withFileTypes: true });
    const artistFolders = items
        .filter(item => item.isDirectory())
        .map(item => item.name);

    console.log(`ğŸ“‹ å…±æœ‰ ${artistFolders.length} ä¸ªæ­Œæ‰‹æ–‡ä»¶å¤¹å¾…æ£€æŸ¥`);

    const totalReport = [];
    let artistsWithDuplicates = 0;
    let totalDuplicateGroups = 0;

    for (const [index, artistName] of artistFolders.entries()) {
        if (skipArtists.includes(artistName)) {
             continue;
        }

        const artistDir = path.join(rootDir, artistName);

        // Progress log every 10 artists or so
        if (index % 10 === 0) {
            process.stdout.write(`\râ³ æ­£åœ¨å¤„ç† (${index + 1}/${artistFolders.length}): ${artistName}...   `);
        }

        const files = findFiles(artistDir, { audioOnly: true });
        if (files.length < 2) continue; // No duplicates possible if < 2 files

        const fileInfos = files.map(f => ({
            path: f,
            size: fs.statSync(f).size,
            name: path.basename(f)
        }));

        const currentArtistDuplicates = [];

        // --- A. Exact Duplicates ---
        const sizeMap = new Map();
        fileInfos.forEach(f => {
            if (!sizeMap.has(f.size)) sizeMap.set(f.size, []);
            sizeMap.get(f.size).push(f);
        });

        for (const [size, group] of sizeMap) {
            if (group.length < 2 || size === 0) continue;
            const hashMap = new Map();
            group.forEach(f => {
                const hash = getFileHash(f.path);
                if (hash) {
                    if (!hashMap.has(hash)) hashMap.set(hash, []);
                    hashMap.get(hash).push(f);
                }
            });
            for (const [hash, sameFiles] of hashMap) {
                if (sameFiles.length > 1) {
                    currentArtistDuplicates.push({
                        type: 'exact',
                        files: sameFiles,
                        size: size
                    });
                }
            }
        }

        // --- B. Semantic Duplicates ---
        // Only check files that are NOT already in exact duplicates groups?
        // Actually, simpler to just run the check and maybe flag them.
        // Or, to avoid double counting, we can ignore files identified as exact dupes?
        // Let's keep it simple: run independent checks, but in the report make sure we distinguish.

        const songMap = new Map();
        fileInfos.forEach(f => {
            // Already filtered in findFiles, but safe to keep check or remove
            // if (!/\.(mp3|m4a|flac|wav|wma|ape)$/i.test(f.name)) return;

            // Pass artistName as default
            const info = parseSongInfo(f.name, artistName);

            // Key is Title only? No, duplicate within "Jay Chou" folder might be "Zhou Jielun - Song" vs "Song".
            // If we use folder name as default artist, then "Song" -> "Jay Chou - Song".
            // So we can group by Title primarily, assuming the folder IS the artist.
            // But what if there is a "Feat. X" song?
            // "Jay Chou - Song A" vs "Jay Chou feat. X - Song A".
            // If we strictly group by "Artist|Title", these might not match.
            // But within a single artist folder, maybe grouping by normalized TITLE is aggressive but useful?
            // Let's stick to "Artist|Title" but with the fallback mechanism, it should catch most.

            // Normalize
            const cleanArtist = info.artist.toLowerCase().replace(/\s+/g, '');
            const cleanTitle = info.title.toLowerCase().replace(/\s+/g, '');

            if (cleanTitle) {
                // If the parsed artist is vaguely similar to the folder artist, unify them?
                // Too complex. Let's just use the key.
                const key = `${cleanArtist}|${cleanTitle}`;
                if (!songMap.has(key)) songMap.set(key, []);
                songMap.get(key).push(f);
            }
        });

        for (const [key, group] of songMap) {
            if (group.length > 1) {
                // Filter out if this group is purely a subset of an exact duplicate group
                // (i.e. if they are exact duplicates, we already reported them).
                // Check if all files in this group have the same Size (quick check).
                const sizes = new Set(group.map(g => g.size));
                if (sizes.size === 1) {
                    // Potential exact duplicate, likely caught by logic A.
                    // To avoid noise, let's skip if they are indeed exact dupes.
                    // But we don't have the hash here easily without re-reading.
                    // Let's just report them as "Possible Duplicate (Same Name)" and let user decide.
                    // Actually, if we want a clean report, better to de-dupe the reports.
                    // But for now, separate sections is fine.
                }

                currentArtistDuplicates.push({
                    type: 'semantic',
                    files: group,
                    key: key
                });
            }
        }

        // If we found anything
        if (currentArtistDuplicates.length > 0) {
            artistsWithDuplicates++;
            totalDuplicateGroups += currentArtistDuplicates.length;

            totalReport.push(`\nğŸ“‚ æ­Œæ‰‹: ${artistName}`);

            currentArtistDuplicates.forEach(d => {
                if (d.type === 'exact') {
                    d.files.sort((a, b) => getScore(b) - getScore(a));
                    totalReport.push(`  ğŸ”’ å®Œå…¨é‡å¤ (Size: ${(d.size/1024/1024).toFixed(2)}MB):`);
                    d.files.forEach((f, i) => {
                        totalReport.push(`     ${i===0?'âœ…':'âŒ'} ${path.relative(artistDir, f.path)}`);
                    });
                } else {
                    d.files.sort((a, b) => getScore(b) - getScore(a));
                    const [artist, title] = d.key.split('|');
                    totalReport.push(`  ğŸµ ç–‘ä¼¼é‡å¤: ${title} (${d.files.length} é¦–)`);
                    d.files.forEach((f, i) => {
                        totalReport.push(`     ${i===0?'â­':'  '} ${path.relative(artistDir, f.path)} (${(f.size/1024/1024).toFixed(2)}MB)`);
                    });
                }
            });
        }
    }

    console.log('\n\nâœ… æ‰«æå®Œæˆï¼');
    console.log(`ğŸ“Š ç»Ÿè®¡:`);
    console.log(`   - æ£€æŸ¥æ­Œæ‰‹: ${artistFolders.length}`);
    console.log(`   - å‘ç°é‡å¤çš„æ­Œæ‰‹: ${artistsWithDuplicates}`);
    console.log(`   - é‡å¤æ–‡ä»¶ç»„æ•°: ${totalDuplicateGroups}`);

    if (totalReport.length > 0) {
        fs.writeFileSync(reportFile, totalReport.join('\n'));
        console.log(`\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆ: ${reportFile}`);
    } else {
        console.log('âœ¨ å®Œç¾ï¼æ²¡æœ‰å‘ç°ä»»ä½•é‡å¤ã€‚');
    }
}

run();
